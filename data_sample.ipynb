{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import libraries '''\n",
    "import numpy as np\n",
    "import laspy\n",
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Version: 1.2\n",
      "Default Point Format: <PointFormat(3, 0 bytes of extra dims)>\n",
      "Number of points: 332123203\n",
      "Scale Factor: [0.00025 0.00025 0.00025]\n",
      "Offset: [-18926.90, -59289.69, 29.79]\n",
      "Min Bounds: [-19115.71, -59507.12, -8.74]\n",
      "Max Bounds: [-18738.09, -59072.26, 68.33]\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "User ID: LASF_Projection\n",
      "Record ID: 34735\n",
      "Description: GeoKeyDirectoryTag (mandatory)\n",
      "<bound method GeoKeyDirectoryVlr.parse_crs of <GeoKeyDirectoryVlr(25 geo_keys)>>\n",
      "--------------------------------------------------\n",
      "User ID: LASF_Projection\n",
      "Record ID: 34736\n",
      "Description: GeoDoubleParamsTag (optional)\n",
      "<GeoDoubleParamsVlr([c_double(6378137.0), c_double(298.257222101), c_double(0.0), c_double(36.0), c_double(139.833333333333), c_double(0.0), c_double(0.0), c_double(0.9999)])>\n",
      "--------------------------------------------------\n",
      "User ID: LASF_Projection\n",
      "Record ID: 34737\n",
      "Description: GeoASCIIParamsTag (optional)\n",
      "<GeoAsciiParamsVlr(['JGD2011 / Japan zone 9|JGD2011 / Japan zone 9|JGD2011|'])>\n",
      "--------------------------------------------------\n",
      "User ID: laszip encoded\n",
      "Record ID: 22204\n",
      "Description: http://laszip.org\n"
     ]
    }
   ],
   "source": [
    "''' Display data structure information '''\n",
    "# Specify file path\n",
    "file_path = [r\"Area0_230713_031852_S0.laz\", r\"Area1_230724_065805_S0.laz\", r\"Area2_230724_054607_S0.laz\", r\"Area3_230724_011027_S0.laz\"][1]\n",
    "\n",
    "# Import data\n",
    "with laspy.open(file_path, mode='r')  as las:\n",
    "\n",
    "    # Print public header\n",
    "    public_header = las.header\n",
    "    print(\"Default Version:\", public_header.version)\n",
    "    print(\"Default Point Format:\", public_header.point_format)\n",
    "    print(\"Number of points:\", public_header.point_count)\n",
    "    print(\"Scale Factor:\", public_header.scales)\n",
    "    print(\"Offset: [{}, {}, {}]\".format(*[format(num, '.2f') for num in public_header.offsets]))\n",
    "    print(\"Min Bounds: [{}, {}, {}]\".format(*[format(num, '.2f') for num in public_header.min]))\n",
    "    print(\"Max Bounds: [{}, {}, {}]\".format(*[format(num, '.2f') for num in public_header.max]))\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Print variabe length records (VLR)\n",
    "    vlrs = las.header.vlrs\n",
    "    for vlr in vlrs:\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        print(\"User ID:\", vlr.user_id)\n",
    "        print(\"Record ID:\", vlr.record_id)\n",
    "        print(\"Description:\", vlr.description)\n",
    "\n",
    "        if vlr.record_id == 34735:\n",
    "            print(vlr.parse_crs)\n",
    "\n",
    "        elif vlr.record_id == 34736:\n",
    "            print(vlr)\n",
    "\n",
    "        elif vlr.record_id == 34737:\n",
    "            print(vlr)\n",
    "\n",
    "    # No extended variable length records (EVLRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total iterations: 333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "333it [01:12,  4.57it/s]\n"
     ]
    }
   ],
   "source": [
    "''' Reduce size of laz randomly '''\n",
    "# Specify file path\n",
    "file_path = [r\"Area0_230713_031852_S0.laz\", r\"Area1_230724_065805_S0.laz\", r\"Area2_230724_054607_S0.laz\", r\"Area3_230724_011027_S0.laz\"][1]\n",
    "\n",
    "# Import point cloud data\n",
    "with laspy.open(file_path, mode='r')  as las:\n",
    "    public_header = las.header\n",
    "    n_points_total = public_header.point_count\n",
    "s_chunk = 1000000\n",
    "randomness = 1000\n",
    "length = int((n_points_total // s_chunk + 1) * (s_chunk / randomness))\n",
    "print(\"total iterations: {}\".format(n_points_total // s_chunk + 1))\n",
    "points_array = np.full((length, 6), np.nan)\n",
    "\n",
    "current_index = 0\n",
    "with laspy.open(file_path) as las:\n",
    "    for chunk in tqdm.tqdm(las.chunk_iterator(s_chunk)):\n",
    "        indices = np.random.choice(len(chunk), size=int(s_chunk/randomness), replace=False)\n",
    "        s_chunk_use = int(s_chunk/randomness)\n",
    "        points_array[current_index:current_index+s_chunk_use, 0] = chunk['X'][indices]\n",
    "        points_array[current_index:current_index+s_chunk_use, 1] = chunk['Y'][indices]\n",
    "        points_array[current_index:current_index+s_chunk_use, 2] = chunk['Z'][indices]\n",
    "        points_array[current_index:current_index+s_chunk_use, 3] = chunk['red'][indices]\n",
    "        points_array[current_index:current_index+s_chunk_use, 4] = chunk['green'][indices]\n",
    "        points_array[current_index:current_index+s_chunk_use, 5] = chunk['blue'][indices]\n",
    "        current_index += s_chunk_use\n",
    "\n",
    "# Remove np.nan data points stored as result of chunk reading\n",
    "points_array = points_array[~np.all(np.isnan(points_array), axis=1)]\n",
    "n_sub_points = len(points_array)\n",
    "\n",
    "# Create a new LAS object\n",
    "public_header.point_count = n_sub_points\n",
    "sub_las = laspy.LasData(public_header)\n",
    "\n",
    "# Populate the LAS object with modified points_array\n",
    "sub_las.points['X'] = points_array[:, 0]\n",
    "sub_las.points['Y'] = points_array[:, 1]\n",
    "sub_las.points['Z'] = points_array[:, 2]\n",
    "sub_las.points['red'] = points_array[:, 3]\n",
    "sub_las.points['green'] = points_array[:, 4]\n",
    "sub_las.points['blue'] = points_array[:, 5]\n",
    "\n",
    "# Export sub_las\n",
    "new_file_path = file_path.replace(\".laz\", r\"_random_1_\" + str(randomness) + r\".laz\")\n",
    "sub_las.write(new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total iterations: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [08:39, 17.31s/it]\n"
     ]
    }
   ],
   "source": [
    "''' Split laz into  '''\n",
    "# Specify file path\n",
    "file_path = [r\"Area0_230713_031852_S0.laz\", r\"Area1_230724_065805_S0.laz\", r\"Area2_230724_054607_S0.laz\", r\"Area3_230724_011027_S0.laz\"][3]\n",
    "\n",
    "points_array = np.empty((0, 6), dtype=float)\n",
    "\n",
    "# Import point cloud data\n",
    "with laspy.open(file_path, mode='r') as las:\n",
    "    public_header = las.header\n",
    "\n",
    "    new_public_header = laspy.header.LasHeader()\n",
    "    new_public_header.version = public_header.version\n",
    "    new_public_header.point_format = public_header.point_format\n",
    "    new_public_header.scales = public_header.scales\n",
    "    new_public_header.offsets = public_header.offsets\n",
    "    new_public_header.vlrs = public_header.vlrs\n",
    "\n",
    "    n_points_total = public_header.point_count\n",
    "    chunk_size = 50_000_000\n",
    "    print(\"total iterations: {}\".format(n_points_total // chunk_size + 1))\n",
    "    \n",
    "    for chunk_index, points in enumerate(tqdm.tqdm(las.chunk_iterator(chunk_size))):\n",
    "        chunk_points_array = np.column_stack((points['X'], points['Y'], points['Z'], points['red'], points['green'], points['blue']))\n",
    "        n_sub_points = len(chunk_points_array)\n",
    "        \n",
    "        new_public_header.point_count = n_sub_points\n",
    "        sub_las = laspy.LasData(new_public_header)\n",
    "\n",
    "        # Populate the LAS object with modified points_array\n",
    "        sub_las.points['X'] = chunk_points_array[:, 0]\n",
    "        sub_las.points['Y'] = chunk_points_array[:, 1]\n",
    "        sub_las.points['Z'] = chunk_points_array[:, 2]\n",
    "        sub_las.points['red'] = chunk_points_array[:, 3]\n",
    "        sub_las.points['green'] = chunk_points_array[:, 4]\n",
    "        sub_las.points['blue'] = chunk_points_array[:, 5]\n",
    "\n",
    "        # Export sub_las\n",
    "        new_file_path = file_path.replace(\".laz\", r\"_split_\" + str(chunk_index) + r\".laz\")\n",
    "        new_file_path = r\"split_laz\\\\\" + new_file_path\n",
    "        sub_las.write(new_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmp_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
